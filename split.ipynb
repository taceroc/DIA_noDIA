{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "#import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 8\n",
      "1\n",
      "Real (0) = 44670 and Bogus (1) = 44766\n",
      "(268308, 51, 51)\n",
      "Final lenght of data = (89436, 51, 153)\n",
      "44670 44766\n",
      "44680 44660\n",
      "Len of data where len(ID_0) = len(ID_1) = 89340\n",
      "Final lenght of train = 67005, Final lenght of test = 22335 \n",
      "67005 22335\n",
      "Save train and test for 8\n",
      "67005 22335\n",
      "Save train and test targets for 8\n",
      "Save train and test IDs for 8\n",
      "[0 1] [11158 11177]\n",
      "[0 1] [33502 33503]\n",
      "Done with 8\n",
      "start 9\n",
      "1\n",
      "Real (0) = 21281 and Bogus (1) = 68155\n",
      "(268308, 51, 51)\n",
      "Final lenght of data = (89436, 51, 153)\n",
      "21281 68155\n",
      "21291 21271\n",
      "Len of data where len(ID_0) = len(ID_1) = 42562\n",
      "Final lenght of train = 31921, Final lenght of test = 10641 \n",
      "31921 10641\n",
      "Save train and test for 9\n",
      "31921 10641\n",
      "Save train and test targets for 9\n",
      "Save train and test IDs for 9\n",
      "[0 1] [5348 5293]\n",
      "[0 1] [15923 15998]\n",
      "Done with 9\n",
      "start 10\n",
      "1\n",
      "Bogus (1) = 4555\n",
      "(13665, 51, 51)\n",
      "Final lenght of data = (4555, 51, 153)\n",
      "0 4555\n",
      "4555 0\n",
      "Len of data where len(ID_0) = len(ID_1) = 4555\n",
      "Final lenght of train = 3416, Final lenght of test = 1139 \n",
      "3416 1139\n",
      "Save train and test for 10\n",
      "3416 1139\n",
      "Save train and test targets for 10\n",
      "Save train and test IDs for 10\n",
      "[1] [1139]\n",
      "[1] [3416]\n",
      "Done with 10\n"
     ]
    }
   ],
   "source": [
    "# for ii in range(11):\n",
    "#      #Create path for diff, srch, temp images\n",
    "#     print('start {}'.format(ii))\n",
    "\n",
    "#     if ii != 10:\n",
    "#         path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path)))\n",
    "#     else:\n",
    "#         path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path10)))                      \n",
    "#     print(len(flist))\n",
    "# for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "pttype = '*'\n",
    "\n",
    "for ii in range(8,11):\n",
    "    flist = []\n",
    "    #Create path for diff, srch, temp images\n",
    "    print('start {}'.format(ii))\n",
    "    if ii != 10:\n",
    "        path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path)))\n",
    "    else:\n",
    "        path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "        flist.append(sorted(glob.glob(path10)))                      \n",
    "    print(len(flist))\n",
    "# for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "    flist = np.concatenate((flist))\n",
    "    \n",
    "    ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "    #extract from .feather file the ID that are on flist\n",
    "    ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "    new_labels = pd.read_feather(ffpath)\n",
    "    current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "    current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "    current_labels.drop_duplicates(inplace=True) \n",
    "    current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "    counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "    #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "    if len(counts_type[0]) == 2:\n",
    "        print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "    if len(counts_type[0]) == 1:\n",
    "        if counts_type[0] == 0:\n",
    "            print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "        else:\n",
    "            print(\"Bogus (1) = {}\".format(counts_type[1][0]))\n",
    "\n",
    "\n",
    "    imlist_dict = {}\n",
    "\n",
    "    # stores the name of the images as a list for ID above\n",
    "    #is a circle because i extract the ID for the flist, buttt\n",
    "    imlist_dict['flist'] = [f for f in flist if int(f.split('/')[-1][4:-5]) in current_labels['ID'].to_numpy()]\n",
    "    #print (len(imlist_dict['flist']))\n",
    "    #print(flist.nbytes)\n",
    "    #del(flist)\n",
    "    imlist_dict[\"imshp\"] = fits.open((imlist_dict[\"flist\"][0]))[0].data.shape #shape row,col\n",
    "    extension=\"fits\"\n",
    "    imdtype = {\"fits\":float, \"gif\":np.uint8, }\n",
    "\n",
    "    #sort as: descending ID and diff, srch, temp\n",
    "    imlist_dict[\"flist\"] = sorted(imlist_dict[\"flist\"], key=lambda s: s.split('/')[-1][:4])\n",
    "    imlist_dict[\"flist\"]= sorted(imlist_dict[\"flist\"], key=lambda s: int(s.split('/')[-1][4:-5]))\n",
    "\n",
    "    #container for data train and data test\n",
    "    data_full = np.zeros((len(imlist_dict[\"flist\"]),imlist_dict[\"imshp\"][0], imlist_dict[\"imshp\"][1]),imdtype[extension])\n",
    "\n",
    "    #fill the container and open images\n",
    "    for i in range(len(imlist_dict[\"flist\"])):\n",
    "        datas = fits.open(''.join(imlist_dict[\"flist\"][i]), memmap=True)\n",
    "        #datas.close()\n",
    "        data_full[i] = datas[0].data\n",
    "        #print(\"{}, path:{}\".format(i,imlist_dict[\"flist\"][i]))\n",
    "        datas.close()\n",
    "\n",
    "    print(data_full.shape)\n",
    "\n",
    "\n",
    "    data_norm = data_full.astype(float)\n",
    "    data_full = None\n",
    "    # # --normalize\n",
    "    # # mean and std for diff images\n",
    "    # # min and max for srch and temp\n",
    "\n",
    "    data_norm[::3] = (data_norm[::3]- data_norm[::3].mean(axis=(1,2), keepdims=True))/data_norm[::3].std(axis=(1,2), keepdims=True) #diff\n",
    "    data_norm[1::3]= (data_norm[1::3]-data_norm[1::3].min(axis=(1,2), keepdims=True))/(data_norm[1::3].max(axis=(1,2), keepdims=True)-data_norm[1::3].min(axis=(1,2), keepdims=True)) #srch\n",
    "    data_norm[2::3]= (data_norm[2::3]-data_norm[2::3].min(axis=(1,2), keepdims=True))/(data_norm[2::3].max(axis=(1,2), keepdims=True)-data_norm[2::3].min(axis=(1,2), keepdims=True)) #temp\n",
    "\n",
    "    #concatenate diff srch temp for the same ID\n",
    "\n",
    "    #final_data = np.zeros((int(len(data_full)//3),imlist_dict[\"imshp\"][0], imlist_dict[\"imshp\"][1]*3))\n",
    "    final_data = np.concatenate((data_norm[::3],data_norm[1::3],data_norm[2::3]), axis = 2)\n",
    "    data_norm = None\n",
    "    print('Final lenght of data = {}'.format(final_data.shape)) \n",
    "\n",
    "    #exxtract the objects  = 0\n",
    "    df_ID_0 = current_labels[current_labels[\"OBJECT_TYPE\"]==0]\n",
    "    #exxtract the objects  = 1\n",
    "    df_ID_1 = current_labels[current_labels[\"OBJECT_TYPE\"]==1]\n",
    "\n",
    "    #the len is the minimun of object 0, and object 1. To have equal data of both\n",
    "    len_each_set = min(len(df_ID_0), len(df_ID_1))\n",
    "    print(len(df_ID_0), len(df_ID_1))\n",
    "\n",
    "    if len_each_set != 0:\n",
    "        if len(df_ID_0) <= len_each_set:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set-10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set+10,random_state = 2).sort_index()\n",
    "        else:\n",
    "            #extract random the number of data classify as 0\n",
    "            index_data_ID0 = df_ID_0.sample(len_each_set+10, random_state = 2).sort_index()\n",
    "            #extract random the number of data classify as 1\n",
    "            index_data_ID1 = df_ID_1.sample(len_each_set-10,random_state = 2).sort_index()\n",
    "        \n",
    "    if len(df_ID_0) == 0:\n",
    "        index_data_ID1 = df_ID_1.sort_index()\n",
    "        index_data_ID0 = df_ID_0\n",
    "        finalIDs = index_data_ID1\n",
    "        #index_data_ID1.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "    if len(df_ID_1) == 0:\n",
    "        index_data_ID0 = df_ID_0.sort_index()\n",
    "        index_data_ID1 = df_ID_1\n",
    "        finalIDs = index_data_ID0\n",
    "        #index_data_ID0.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    finalIDs = index_data_ID0.append(index_data_ID1)\n",
    "    #finalIDs.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "    print(len(index_data_ID1),len(index_data_ID0))\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID0 = index_data_ID0.index.to_numpy()\n",
    "\n",
    "    #convert index to numpy to iterate\n",
    "    index_ID1 = index_data_ID1.index.to_numpy()\n",
    "\n",
    "    #concatenate both index\n",
    "    indexes = sorted(np.concatenate((index_ID0, index_ID1)))\n",
    "\n",
    "    #extract the data from the index given above, of the complete data, where 0 and 1 are not equal\n",
    "    equal_type_data = len(indexes)\n",
    "    print(\"Len of data where len(ID_0) = len(ID_1) = {}\".format(equal_type_data))\n",
    "\n",
    "    #75% is for training\n",
    "    #25% testing\n",
    "    train_len = int(equal_type_data*0.75)\n",
    "    test_len = equal_type_data  - int(equal_type_data*0.75)\n",
    "    print('Final lenght of train = {}, Final lenght of test = {} '.format(train_len, test_len))\n",
    "\n",
    "    import random\n",
    "    random.seed(4)\n",
    "    random_index = random.sample(range(0, equal_type_data), train_len)\n",
    "\n",
    "    train = np.array([final_data[i] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test = np.array([final_data[i] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "    \n",
    "    print(len(train),len(test))\n",
    "    np.save('../data/data_split_n/train%d'%ii+'.npy', train)\n",
    "    np.save('../data/data_split_n/test%d'%ii+'.npy', test)\n",
    "    print('Save train and test for {}'.format(ii))\n",
    "\n",
    "\n",
    "    # #extracting the label 0 or 1\n",
    "    targets = [current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes]\n",
    "\n",
    "    #split the targets\n",
    "    train_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "\n",
    "    train_ID = np.array([current_labels.iloc[i][\"ID\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "    test_ID = np.array([current_labels.iloc[i][\"ID\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "    print(len(train_ID),len(test_ID))\n",
    "\n",
    "    np.save('../data/data_split_n/train_targ_%d'%ii+'.npy', train_targ)\n",
    "    np.save('../data/data_split_n/test_targ_%d'%ii+'.npy', test_targ)\n",
    "    print('Save train and test targets for {}'.format(ii))\n",
    "\n",
    "    np.save('../data/data_split_n/train_ID_%d'%ii+'.npy', train_ID)\n",
    "    np.save('../data/data_split_n/test_ID_%d'%ii+'.npy', test_ID)\n",
    "    print('Save train and test IDs for {}'.format(ii))\n",
    "\n",
    "    (unique, counts) = np.unique(test_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "\n",
    "    (unique, counts) = np.unique(train_targ, return_counts=True)\n",
    "    print(unique, counts)\n",
    "    \n",
    "    print('Done with {}'.format(ii))\n",
    "    \n",
    "    flist = None\n",
    "    final_data = None\n",
    "    train = None\n",
    "    test = None\n",
    "    imlist_dict = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in range(1,2):\n",
    "#     pttype = '*'\n",
    "#     flist = []\n",
    "#     print('start {}'.format(ii))\n",
    "#     path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "#     flist.append(sorted(glob.glob(path)))\n",
    "#     flist = np.concatenate((flist))\n",
    "\n",
    "#     #Extract the ID of the flist\n",
    "#     ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "#     #extract from .feather file the ID that are on flist\n",
    "#     ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "#     new_labels = pd.read_feather(ffpath)\n",
    "#     current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "#     current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "#     current_labels.drop_duplicates(inplace=True) \n",
    "#     current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "#     counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "#     #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "#     if len(counts_type[0]) == 2:\n",
    "#         print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "#     if len(counts_type[0]) == 1:\n",
    "#         if counts_type[0] == 0:\n",
    "#             print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "#         else:\n",
    "#             print(\"Bogus (1) = {}\".format(counts_type[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pttype = '*'\n",
    "\n",
    "# for ii in range(4,5):\n",
    "#     flist = []\n",
    "#     #Create path for diff, srch, temp images\n",
    "#     print('start {}'.format(ii))\n",
    "#     if ii != 10:\n",
    "#         path = os.path.join(configs[\"dpath\"],'stamps%d'%ii,'SNWG','Archive','*','Y1','*','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path)))\n",
    "#     else:\n",
    "#         path10 = os.path.join(configs[\"dpath\"],'stamps10','*',pttype + '*.fits')\n",
    "#         flist.append(sorted(glob.glob(path10)))                      \n",
    "#     print(len(flist))\n",
    "# # for i in [\"20130829\",\"20130831\", \"20130901\"]:\n",
    "# #     path = os.path.join(configs[\"dpath\"],'stamps1','SNWG','Archive','*','Y1',i,'*',pttype + '*.fits')\n",
    "# #     flist.append(sorted(glob.glob(path)))\n",
    "\n",
    "#     flist = np.concatenate((flist))\n",
    "    \n",
    "#     ID =[int(f.split('/')[-1][4:-5]) for f in flist]\n",
    "\n",
    "#     #extract from .feather file the ID that are on flist\n",
    "#     ffpath = os.path.join(configs[\"dpath\"], \"autoscan_features.3.feather\") #this .feather file contain only the ID and OBJECT_TYPE for the images that I have on \n",
    "#     new_labels = pd.read_feather(ffpath)\n",
    "#     current_labels = new_labels[new_labels[\"ID\"].isin(ID)]\n",
    "#     current_labels = current_labels[[\"ID\", \"OBJECT_TYPE\"]]\n",
    "#     current_labels.drop_duplicates(inplace=True) \n",
    "#     current_labels = current_labels.sort_values(by= [\"ID\"]).reset_index(drop=True)\n",
    "#     counts_type = np.unique(current_labels['OBJECT_TYPE'], return_counts=True)\n",
    "#     #how_many = {\"Real (0)\":counts_type[1][0], \"Bogus (1)\": counts_type[1][1] }\n",
    "\n",
    "#     if len(counts_type[0]) == 2:\n",
    "#         print(\"Real (0) = {} and Bogus (1) = {}\".format(counts_type[1][0], counts_type[1][1]))\n",
    "#     if len(counts_type[0]) == 1:\n",
    "#         if counts_type[0] == 0:\n",
    "#             print(\"Real (0) = {}\".format(counts_type[1][0]))\n",
    "#         else:\n",
    "#             print(\"Bogus (1) = {}\".format(counts_type[1][0]))\n",
    "\n",
    "#     #exxtract the objects  = 0\n",
    "#     df_ID_0 = current_labels[current_labels[\"OBJECT_TYPE\"]==0]\n",
    "#     #exxtract the objects  = 1\n",
    "#     df_ID_1 = current_labels[current_labels[\"OBJECT_TYPE\"]==1]\n",
    "\n",
    "#     #the len is the minimun of object 0, and object 1. To have equal data of both\n",
    "#     len_each_set = min(len(df_ID_0), len(df_ID_1))\n",
    "#     print(len(df_ID_0), len(df_ID_1))\n",
    "\n",
    "#     if len_each_set != 0:\n",
    "#         if len(df_ID_0) <= len_each_set:\n",
    "#             #extract random the number of data classify as 0\n",
    "#             index_data_ID0 = df_ID_0.sample(len_each_set-10, random_state = 2).sort_index()\n",
    "#             #extract random the number of data classify as 1\n",
    "#             index_data_ID1 = df_ID_1.sample(len_each_set+10,random_state = 2).sort_index()\n",
    "#         else:\n",
    "#             #extract random the number of data classify as 0\n",
    "#             index_data_ID0 = df_ID_0.sample(len_each_set+10, random_state = 2).sort_index()\n",
    "#             #extract random the number of data classify as 1\n",
    "#             index_data_ID1 = df_ID_1.sample(len_each_set-10,random_state = 2).sort_index()\n",
    "        \n",
    "#     if len(df_ID_0) == 0:\n",
    "#         index_data_ID1 = df_ID_1.sort_index()\n",
    "#         index_data_ID0 = df_ID_0\n",
    "#         finalIDs = index_data_ID1\n",
    "#         #index_data_ID1.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "#     if len(df_ID_1) == 0:\n",
    "#         index_data_ID0 = df_ID_0.sort_index()\n",
    "#         index_data_ID1 = df_ID_1\n",
    "#         finalIDs = index_data_ID0\n",
    "#         #index_data_ID0.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "#     finalIDs = index_data_ID0.append(index_data_ID1)\n",
    "#     #finalIDs.to_pickle('ID_stamps%d'%ii+'.pkl')\n",
    "\n",
    "#     print(len(index_data_ID1),len(index_data_ID0))\n",
    "\n",
    "#     #convert index to numpy to iterate\n",
    "#     index_ID0 = index_data_ID0.index.to_numpy()\n",
    "\n",
    "#     #convert index to numpy to iterate\n",
    "#     index_ID1 = index_data_ID1.index.to_numpy()\n",
    "\n",
    "#     #concatenate both index\n",
    "#     indexes = sorted(np.concatenate((index_ID0, index_ID1)))\n",
    "\n",
    "#     #extract the data from the index given above, of the complete data, where 0 and 1 are not equal\n",
    "#     equal_type_data = len(indexes)\n",
    "#     print(\"Len of data where len(ID_0) = len(ID_1) = {}\".format(equal_type_data))\n",
    "\n",
    "#     #75% is for training\n",
    "#     #25% testing\n",
    "#     train_len = int(equal_type_data*0.70)\n",
    "#     test_len = equal_type_data  - int(equal_type_data*0.70)\n",
    "#     print('Final lenght of train = {}, Final lenght of test = {} '.format(train_len, test_len))\n",
    "\n",
    "#     import random\n",
    "#     random.seed(4)\n",
    "#     random_index = random.sample(range(0, equal_type_data), train_len)\n",
    "\n",
    "#     # #extracting the label 0 or 1\n",
    "#     targets = [current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes]\n",
    "\n",
    "#     #split the targets\n",
    "#     train_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "#     test_targ = np.array([current_labels.iloc[i][\"OBJECT_TYPE\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "\n",
    "#     train_ID = np.array([current_labels.iloc[i][\"ID\"] for i in [indexes[i] for i in sorted(random_index)]])\n",
    "#     test_ID = np.array([current_labels.iloc[i][\"ID\"] for i in indexes if i not in [indexes[i] for i in sorted(random_index)]])\n",
    "#     print(len(train_ID),len(test_ID))\n",
    "\n",
    "#     np.save('../data/data_split_n/train_targ_%d'%ii+'.npy', train_targ)\n",
    "#     np.save('../data/data_split_n/test_targ_%d'%ii+'.npy', test_targ)\n",
    "#     print('Save train and test targets for {}'.format(ii))\n",
    "\n",
    "#     np.save('../data/data_split_n/train_ID_%d'%ii+'.npy', train_ID)\n",
    "#     np.save('../data/data_split_n/test_ID_%d'%ii+'.npy', test_ID)\n",
    "#     print('Save train and test IDs for {}'.format(ii))\n",
    "\n",
    "#     (unique, counts) = np.unique(test_targ, return_counts=True)\n",
    "#     print(unique, counts)\n",
    "\n",
    "#     (unique, counts) = np.unique(train_targ, return_counts=True)\n",
    "#     print(unique, counts)\n",
    "    \n",
    "#     print('Done with {}'.format(ii))\n",
    "    \n",
    "#     flist = None\n",
    "#     final_data = None\n",
    "#     train = None\n",
    "#     test = None\n",
    "#     imlist_dict = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tff-TAC",
   "language": "python",
   "name": "tff-tac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
